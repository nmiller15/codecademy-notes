# Asymtotic Notation

## Why Asymtotic Notation?

In programming it is important to be able to evaluate the efficiency of a program. Even though calculations and programs may seem to run instantaneously on a computer, when they run on larger and larger data sets, efficient programming methods become essential to successful programs.

We evaluate the efficiency of a program based on its **runtime**. But, since this varies from computer to computer and language to langage, we will calculate a runtime based on the relationship between **the number of instructions per input**.

*N -* The variable representing an input

*M* - A variable that optionally represents a second input

In general, we will want to evaluate the runtime of the worst case scenerio, so that we know the longest possible runtime of the program.

## What is Asymtotic Notation?

Asymtotic Notation is Big O notation, there are a couple of variations on it, but Big O is the one you hear most about. Asymtotic notation is concerned with the trend of the change in executions as inputs (N) increase.

Asymtotic referrs to the the approaching of infinity. Since we are approaching infinity, constants don’t really matter, so in our expressions we will drop them and be left only with N when we simplify. We truly only care about the dominant term in the expression.

$$
5N^2 + 2N + 4 
  \text{   becomes   } \\
N^2 + N \text { becomes } \\
N^2
$$

There is also **execution count** which is more specific than Big O, but also less helpful when comparing programs.

Big O, asymtotic notation allows us to formalize the fuzzy counting of the efficency of an algorithm.

## Big Theta (Θ)

When there are only single runtime cases for a function, we will use theta (Θ) to express it. Take the following examples:

```plaintext
Function with input that is a list of size N
  For each value in list:
    Print the value
```

Since the size of the input and the size of the instructions that the computer must perform are the same here (for a list of any size, each value in the list will be printed), *and* there is only one possible outcome of this function, the runtime of the program is **Θ(N)**

```plaintext
Function that has integer input N:
  Set a count variable to 0
  Loop while N is not equal to 1:
    Increment count
    N = N/2
  Return count
```

Here, the computer performs less of the size of N the larger it gets, specifically in a logarithmic way. There is also only one possible outcome of this function, so this program's runtime is **Θ(log N)**

## Common Runtimes

- **Θ(1)** - Constant runtime, the program will always do the same thing, regarless of input
- **Θ(log N)** - logarithmic runtime, typical of search algorithms
- **Θ(N)** - linear runtime, when you have to iterate through an entire dataset
- **Θ(N * logN)** - typical of sorting algorithms
- **Θ(N^2)** - polynomial runtime, if raise to 2 then it’s quadratic, searching two-dimentional datasets or nested loops
- **Θ(2^N)** - exponential runtime, recursion
- **Θ(N!)** - factorial runtime, when you need to find the permutations of something

## Big Omega (Ω) and Big O (O)

When a program has different possible runtimes, or the best case and worst cases are different, we will describe the best case with Big Omega and the worst case with Big O.

Take the following example:

```plaintext
Function with input that is a list of size N:
  For each value in the list:
    If value is equal to 12:
      Return True
  Return false
```

In this case, the value of the input determines how long the program will run. Where in the list is the value 12? If it’s in the first position, then it will only iterate one time! That’s our best case, but if it is located at the back, then it will iterate the number of the size of the list.

- The best case is constant - Θ(1)
- The worst case is linear - Θ(N)
- So the runtime is - Ω(1) *and* O(N)

Since we are always preparing for the worst case, the Big O of the function is usually most relevant for our discussions. In technical interview, most interviewers will only ask you to determine the Big O of the problem.

## Adding Runtimes

How can we figure out the runtimes of complex programs with many steps?

```plaintext
Function that takes a positive integer N:
  Set a variable i equal to 1
  Loop until i is equal to N:
    Print i
    Increment i

  Set a count variable to 0
  Loop while N is not equal to 1:
    Increment count
    N = N / 2
  Return count
```

- The first loop iterates the entire value of N, or the whole size so it is Θ(N)
- The second loop is logarithmic, so it is Θ(log N)

To find the collective runtime we can add these together: Θ(N) + Θ(log N)

However, when we are adding, we only care about the slowest portion, since the slowest portion will still have to execute, so this function is Θ(N).

It is also appropriate to label this as O(N), in big O notation rather than theta. This is because, if the program runs in Θ(N) in every case, it will have the same runtime as the worst runtime. Most of the time, people will only use big O, since we only care about the worst case, but it’s good to know the differences between the three notations.

